\documentclass[10pt,twocolumn,letterpaper]{article}
\usepackage{cvpr}
\usepackage{graphicx}
\usepackage{amsmath,amssymb}
\usepackage{booktabs}
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}
\usepackage[capitalize]{cleveref}

\def\cvprPaperID{****}
\def\confName{CVPR}
\def\confYear{2025}

\title{Introduction to machine learning competition project}

\author{Tommaso Grotto\\\texttt{tommaso.grotto@studenti.unitn.it}\\\and Stefano Murtas\\\texttt{stefano.murtas@studenti.unitn.it}}

\begin{document}
\maketitle

\section{Introduction}
\subsection{Task Description}
The goal of this task was to implement an image retrieval system that, given a query image, returns the top-k most similar images from a separate gallery set. This type of similarity search is widely used in applications such as recommendation systems, content-based image retrieval, and facial recognition. The challenge required designing an approach that performs well under strict time constraints, using only the provided training and test sets with no external labels for the test set.

\subsection{Overview of Approaches}
To solve this task, we explored a broad range of deep learning models, from classical CNN-based feature extractors to modern contrastive learning frameworks and fine-tuned multimodal models. Our pipeline generally followed this structure:

\begin{enumerate}
  \item Feature Extraction: Use a pre-trained or fine-tuned model to extract embeddings from gallery and query images.
  \item Normalization \& Similarity: Normalize embeddings and compute cosine similarity between query and gallery features.
  \item Top-k Retrieval: Return the most similar gallery images based on similarity scores.
\end{enumerate}

The main models we considered included:
\begin{itemize}
  \item ResNet (both pre-trained and fine-tuned)
  \item CLIP (zero-shot and fine-tuned variants)
  \item Triplet Network
  \item ArcFace
  \item SimCLR (self-supervised contrastive learning)
  \item Siamese Networks
  \item HuggingFace vision models
  \item Timm models
  \item FAISS for scalable similarity search
\end{itemize}

\subsection{Summary of Results}
Across our experiments, we found that CLIP offered the most consistent performance in top-k retrieval accuracy, with a good speed of processing. The other models performed poorly in the accuracy part when compared to CLIP, or they took too much time for the training aspect. Results were evaluated both quantitatively (top-k accuracy) and qualitatively (visual inspection of results).

\section{Models Considered}
In this section, we describe the models we explored for the image retrieval task, including their theoretical underpinnings and relevant literature references. The selection spans classical CNNs, self-supervised learning, metric learning techniques, and state-of-the-art multimodal models.

\subsection{ResNet (Pretrained \& Fine-Tuned)}
\textbf{Description:} ResNet (Residual Networks) introduced by He et al. (2016) is a deep convolutional architecture that uses skip connections to alleviate the vanishing gradient problem in deep networks. We employed the ResNet50 variant, which contains 50 layers, and is widely used for transfer learning, and the ResNet18 as a lightweight CNN baseline. We used both:
\begin{itemize}
  \item Pretrained: Used directly as a feature extractor by removing the final classification layer.
  \item Fine-Tuned: Retrained on the labeled training set to adapt feature representations to the specific domain.
\end{itemize}
\textbf{Reference:} He, K., Zhang, X., Ren, S., \& Sun, J. (2016). Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 770-778).

\subsection{CLIP (Contrastive Language--Image Pretraining)}
\textbf{Description:} CLIP (Radford et al., 2021) is a multimodal model trained to align image and text representations using contrastive learning. It uses a ViT (Vision Transformer) or ResNet backbone and is trained on 400M (image, text) pairs. We used both:
\begin{itemize}
  \item Zero-shot CLIP (pretrained without modification)
  \item Fine-Tuned CLIP using image--label text pairs from our dataset.
\end{itemize}
\textbf{Reference:} Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., ... \& Sutskever, I. (2021, July). Learning transferable visual models from natural language supervision. In International conference on machine learning (pp. 8748-8763). PmLR.

\subsection{Triplet Network}
\textbf{Description:} Triplet networks are trained using a triplet loss that pulls an anchor image closer to a positive (same class) and farther from a negative (different class). This encourages the model to learn an embedding space where semantic similarity translates to spatial proximity.

\textbf{Reference:} Schroff, F., Kalenichenko, D., \& Philbin, J. (2015). Facenet: A unified embedding for face recognition and clustering. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 815-823).

\subsection{ArcFace}
\textbf{Description:} ArcFace introduces an additive angular margin loss to improve inter-class separability and intra-class compactness. Instead of using standard softmax, it operates on the angle between embeddings and class weights.

\textbf{Reference:} Deng, J., Guo, J., Xue, N., \& Zafeiriou, S. (2019). Arcface: Additive angular margin loss for deep face recognition. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 4690-4699).

\subsection{SimCLR (Simple Framework for Contrastive Learning)}
\textbf{Description:} SimCLR is a self-supervised contrastive learning framework where models are trained to bring augmentations of the same image closer and push apart different images. No labels are required -- just strong data augmentation and a contrastive loss.

\textbf{Reference:} Chen, T., Kornblith, S., Norouzi, M., \& Hinton, G. (2020, November). A simple framework for contrastive learning of visual representations. In International conference on machine learning (pp. 1597-1607). PmLR.

\subsection{Siamese Networks}
\textbf{Description:} Siamese networks use two identical subnetworks to encode a pair of images and apply a contrastive loss to minimize the distance for similar pairs and maximize it for dissimilar ones. We trained both single and double fine-tuned Siamese networks.

\textbf{Reference:} Koch, G., Zemel, R., \& Salakhutdinov, R. (2015, July). Siamese neural networks for one-shot image recognition. In ICML deep learning workshop (Vol. 2, No. 1, pp. 1-30).

\subsection{FAISS (Facebook AI Similarity Search)}
\textbf{Description:} FAISS is a library for efficient similarity search and clustering of dense vectors. It is used post-feature-extraction to retrieve top-k similar images quickly, especially when scaling to large gallery sets.

\textbf{Reference:} Johnson, J., Douze, M., \& J\'egou, H. (2019). Billion-scale similarity search with GPUs. IEEE Transactions on Big Data, 7(3), 535-547.

\subsection{Other Models (HuggingFace ViT \& Timm)}
We briefly experimented with vision transformer models from the Hugging Face transformers library and timm (PyTorch Image Models), using them primarily as pretrained feature extractors. While these models were not fine-tuned, they offered competitive baseline embeddings.

\section{Evaluation}
We evaluated our models using top-k accuracy, which measures the proportion of query images whose correct match appears among the top-k retrieved gallery images. Below are the results obtained during the competition day.

\begin{table}[ht]
\centering
\caption{Models' accuracy in the image retrieval task. ND: not deployed in the competition.}
\begin{tabular}{ll}
\toprule
\textbf{Model} & \textbf{Accuracy} \\
\midrule
CLIP (Zero-Shot) & 345.57 \\
Siamese Networks & 46.52 \\
HuggingFace ViT & 43.37 \\
ResNet18 & 30.72 \\
ResNet50 (Pretrained) & 30.58 \\
FAISS & 22.75 \\
ResNet50 (Fine-Tuned) & ND \\
CLIP (Fine-Tuned) & ND \\
Triplet Network & ND \\
ArcFace & ND \\
SimCLR & ND \\
Timm & ND \\
\bottomrule
\end{tabular}
\end{table}

From a quantitative perspective, the CLIP (Zero-Shot) model clearly stood out. It demonstrated exceptional ability to retrieve relevant matches, achieving the highest score without any task-specific training. This highlights the power of large-scale multimodal pretraining for zero-shot generalization.

In contrast, the other models performed notably worse. In order of accuracy, we have Siamese Networks, HuggingFace ViT, ResNet-based models, and FAISS. A key challenge we faced was that several of our promising models (e.g., ArcFace, Triplet Network, SimCLR) were not deployable within the competition time window, due to their computational demands or longer training requirements.

\section{Discussion}
This image similarity task provided a valuable opportunity to experiment with a wide spectrum of models under realistic constraints, such as limited training time and compute availability. Through this experience, we gained key insights into model performance, efficiency, and robustness in the context of content-based image retrieval.

One of the clearest takeaways was the outstanding performance of the CLIP (Zero-Shot) model. Without any task-specific fine-tuning, CLIP significantly outperformed all other approaches. Its success illustrates the practical benefits of large-scale pretraining on diverse multimodal data, and highlights how foundation models can generalize remarkably well to downstream retrieval tasks.

Other models, such as Siamese Networks and HuggingFace ViT, demonstrated a not so good performance, although this is balanced by their lower resource demands and faster training time. These models might be considered as alternatives in time-constrained settings where full fine-tuning or training from scratch is not feasible.

Several of our most theoretically promising models, including ArcFace, Triplet Networks, and SimCLR, were not deployable within the competition's two-hour time frame. This reflects an important trade-off between model expressiveness and practical usability. While these models often achieve excellent results in offline experiments, their training time and tuning requirements make them less suitable for time-critical environments unless prepared in advance.

The integration of FAISS for similarity search highlighted another important consideration: even with efficient indexing, the overall system performance is still limited by the quality of the underlying embeddings. This reinforced the importance of focusing on robust, generalizable feature extraction as a foundation for retrieval performance.

This challenge emphasized not just model accuracy, but also the importance of efficiency and strategic model selection in applied machine learning.

\section{Workload Table}
In the following table, we report the contributions of each member. The ‘Models’ row reports the models that each member created that were used in the competition day, the 'Models ND' row reports the models each member created but that were not employed, the ‘Report’ row represents the contribution to the writing of the report, and the ‘Repository’ row indicates the contribution to the creation of the GitHub repository for this project.

\begin{table}[ht]
\centering
\caption{Members' contribution to the competition project.}
\begin{tabular}{ll}
\toprule
\textbf{Members and contributions} \\
\midrule
\textbf{Tommaso Grotto} & \\
\quad Models: CLIP, HuggingFace ViT, ResNet50 & \\
\quad Models ND: Triplet Network, ArcFace SimCLR & \\
\quad Report: 70\% & \\
\quad Repository: 30\% & \\
\addlinespace
\textbf{Stefano Murtas} & \\
\quad Models: ResNet18, FAISS, Siamese Networks & \\
\quad Models ND: Timm & \\
\quad Report: 30\% & \\
\quad Repository: 70\% & \\
\bottomrule
\end{tabular}
\end{table}

The code used for this project is available at:

\url{https://github.com/smurtas/ML-STF.git}

\end{document}

