{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f68f3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet50  Modello 1\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def get_image_paths(folder):\n",
    "    return [os.path.join(folder, f) for f in os.listdir(folder) if f.endswith('.jpg')]\n",
    "\n",
    "class ImageFolderDataset(Dataset):\n",
    "    def __init__(self, paths, transform=None):\n",
    "        self.paths = paths\n",
    "        self.transform = transform or T.Compose([\n",
    "            T.Resize(256),\n",
    "            T.CenterCrop(224),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.paths[idx]\n",
    "        img = Image.open(path).convert('RGB')\n",
    "        return self.transform(img), path\n",
    "\n",
    "# Load pretrained ResNet50 and remove last layer\n",
    "model = torchvision.models.resnet50(pretrained=True)\n",
    "model = torch.nn.Sequential(*list(model.children())[:-1])\n",
    "model.eval().cuda()\n",
    "\n",
    "# Paths\n",
    "gallery_folder = 'Data_example/test/gallery'\n",
    "query_folder   = 'Data_example/test/query'\n",
    "\n",
    "gallery_paths = get_image_paths(gallery_folder)\n",
    "query_paths   = get_image_paths(query_folder)\n",
    "\n",
    "# DataLoaders\n",
    "batch_size = 32\n",
    "gallery_loader = DataLoader(ImageFolderDataset(gallery_paths), batch_size=batch_size, shuffle=False)\n",
    "query_loader   = DataLoader(ImageFolderDataset(query_paths),   batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Extract features\n",
    "@torch.no_grad()\n",
    "def extract_features(loader):\n",
    "    feats, paths = [], []\n",
    "    for imgs, pths in loader:\n",
    "        imgs = imgs.cuda()\n",
    "        out = model(imgs).squeeze(-1).squeeze(-1)  # (B,2048)\n",
    "        feats.append(out.cpu().numpy())\n",
    "        paths.extend(pths)\n",
    "    feats = np.vstack(feats)\n",
    "    return feats, paths\n",
    "\n",
    "gallery_feats, gallery_paths = extract_features(gallery_loader)\n",
    "query_feats, query_paths     = extract_features(query_loader)\n",
    "\n",
    "# Normalize\n",
    "gallery_feats = gallery_feats / np.linalg.norm(gallery_feats, axis=1, keepdims=True)\n",
    "query_feats   = query_feats   / np.linalg.norm(query_feats,   axis=1, keepdims=True)\n",
    "\n",
    "# Compute cosine similarities and retrieve top-k\n",
    "k = 5  # set as needed\n",
    "results = []\n",
    "for q_feat, q_path in zip(query_feats, query_paths):\n",
    "    sims = gallery_feats.dot(q_feat)\n",
    "    topk_idx = np.argsort(-sims)[:k]\n",
    "    topk_paths = [gallery_paths[i] for i in topk_idx]\n",
    "    results.append({\n",
    "        'filename': q_path,\n",
    "        'gallery_images': topk_paths\n",
    "    })\n",
    "\n",
    "# Save to JSON\n",
    "with open('submission1.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print('Saved submission.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbdda93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ResNet50 fine-tuned - modello 2\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn, optim\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Data transforms\n",
    "transform = T.Compose([\n",
    "    T.Resize(256),\n",
    "    T.CenterCrop(224),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load training data\n",
    "train_dir = 'Data_example/training'\n",
    "train_dataset = ImageFolder(train_dir, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Load pretrained model\n",
    "model = torchvision.models.resnet50(pretrained=True)\n",
    "num_features = model.fc.in_features\n",
    "model.fc = nn.Linear(num_features, len(train_dataset.classes))  # Replace classifier\n",
    "model = model.cuda()\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Training loop\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.cuda(), labels.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} | Loss: {total_loss/len(train_loader):.4f} | Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "# Save model\n",
    "torch.save(model.state_dict(), 'resnet50_finetuned.pth')\n",
    "print(\"✅ Fine-tuned model saved as 'resnet50_finetuned.pth'\")\n",
    "\n",
    "# Load fine-tuned ResNet50\n",
    "model = torchvision.models.resnet50(pretrained=False)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = torch.nn.Linear(num_ftrs, 3)  # same structure as during training\n",
    "model.load_state_dict(torch.load('resnet50_finetuned.pth'))  # load weights\n",
    "model.fc = torch.nn.Identity()  # remove classification head after loading\n",
    "model.eval().cuda()\n",
    "\n",
    "\n",
    "# Image transforms\n",
    "transform = T.Compose([\n",
    "    T.Resize(256),\n",
    "    T.CenterCrop(224),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Dataset for inference\n",
    "class ImageFolderDataset(Dataset):\n",
    "    def __init__(self, paths, transform):\n",
    "        self.paths = paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        return self.transform(image), img_path\n",
    "\n",
    "# Get all image paths\n",
    "def get_image_paths(folder):\n",
    "    return [os.path.join(folder, f) for f in os.listdir(folder) if f.endswith('.jpg')]\n",
    "\n",
    "gallery_folder = 'Data_example/test/gallery'\n",
    "query_folder   = 'Data_example/test/query'\n",
    "\n",
    "gallery_paths = get_image_paths(gallery_folder)\n",
    "query_paths = get_image_paths(query_folder)\n",
    "\n",
    "# DataLoaders\n",
    "batch_size = 32\n",
    "gallery_loader = DataLoader(ImageFolderDataset(gallery_paths, transform), batch_size=batch_size, shuffle=False)\n",
    "query_loader = DataLoader(ImageFolderDataset(query_paths, transform), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Feature extraction\n",
    "@torch.no_grad()\n",
    "def extract_features(loader):\n",
    "    feats, paths = [], []\n",
    "    for imgs, pths in loader:\n",
    "        imgs = imgs.cuda()\n",
    "        out = model(imgs).squeeze(-1).squeeze(-1)  # Shape: (B, 2048)\n",
    "        feats.append(out.cpu().numpy())\n",
    "        paths.extend(pths)\n",
    "    return np.vstack(feats), paths\n",
    "\n",
    "gallery_feats, gallery_paths = extract_features(gallery_loader)\n",
    "query_feats, query_paths = extract_features(query_loader)\n",
    "\n",
    "# Normalize features\n",
    "gallery_feats = gallery_feats / np.linalg.norm(gallery_feats, axis=1, keepdims=True)\n",
    "query_feats = query_feats / np.linalg.norm(query_feats, axis=1, keepdims=True)\n",
    "\n",
    "# Retrieve top-k gallery images per query\n",
    "k = 5  # You can adjust this\n",
    "results = []\n",
    "for q_feat, q_path in zip(query_feats, query_paths):\n",
    "    sims = gallery_feats.dot(q_feat)\n",
    "    topk_idx = np.argsort(-sims)[:k]\n",
    "    topk_paths = [gallery_paths[i] for i in topk_idx]\n",
    "    results.append({\n",
    "        'filename': q_path,\n",
    "        'gallery_images': topk_paths\n",
    "    })\n",
    "\n",
    "# Save results\n",
    "with open('submission2.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print('✅ Saved retrieval results to submission.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352acbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clip - modello 3\n",
    "\n",
    "#pip install torch torchvision ftfy regex tqdm\n",
    "#pip install git+https://github.com/openai/CLIP.git\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the CLIP model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "# Function to load and preprocess images\n",
    "def load_images_from_folder(folder):\n",
    "    image_paths = []\n",
    "    images = []\n",
    "    for filename in os.listdir(folder):\n",
    "        if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif')):\n",
    "            path = os.path.join(folder, filename)\n",
    "            try:\n",
    "                image = preprocess(Image.open(path).convert(\"RGB\"))\n",
    "                images.append(image)\n",
    "                image_paths.append(path)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading image {path}: {e}\")\n",
    "    return image_paths, images\n",
    "\n",
    "# Load gallery images\n",
    "gallery_folder = \"Data_example/test/gallery\"\n",
    "gallery_paths, gallery_images = load_images_from_folder(gallery_folder)\n",
    "\n",
    "# Compute gallery features\n",
    "gallery_images_tensor = torch.stack(gallery_images).to(device)\n",
    "with torch.no_grad():\n",
    "    gallery_features = model.encode_image(gallery_images_tensor)\n",
    "    gallery_features /= gallery_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "# Load query images\n",
    "query_folder = \"Data_example/test/query\"\n",
    "query_paths, query_images = load_images_from_folder(query_folder)\n",
    "\n",
    "# Compute query features\n",
    "query_images_tensor = torch.stack(query_images).to(device)\n",
    "with torch.no_grad():\n",
    "    query_features = model.encode_image(query_images_tensor)\n",
    "    query_features /= query_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "# Compute similarity and retrieve top-k matches\n",
    "top_k = 5\n",
    "results = []\n",
    "for i, query_feature in enumerate(query_features):\n",
    "    similarities = (gallery_features @ query_feature.unsqueeze(1)).squeeze(1)\n",
    "    top_k_indices = similarities.topk(top_k).indices\n",
    "    top_k_paths = [gallery_paths[idx] for idx in top_k_indices]\n",
    "    results.append({\n",
    "        \"filename\": query_paths[i],\n",
    "        \"gallery_images\": top_k_paths\n",
    "    })\n",
    "\n",
    "# Save results to JSON\n",
    "with open(\"submission3.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(\"✅ Retrieval results saved to submission.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c91c489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HuggingFace - modello 4 \n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import faiss\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from transformers import ViTFeatureExtractor, ViTModel\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load pre-trained model and feature extractor\n",
    "model_name = 'google/vit-base-patch16-224-in21k'\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained(model_name)\n",
    "model = ViTModel.from_pretrained(model_name).to(device)\n",
    "model.eval()\n",
    "\n",
    "# Define image preprocessing\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std)\n",
    "])\n",
    "\n",
    "# Load and preprocess images\n",
    "def load_images_from_folder(folder_path):\n",
    "    image_paths = []\n",
    "    images = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif')):\n",
    "            path = os.path.join(folder_path, filename)\n",
    "            try:\n",
    "                image = Image.open(path).convert('RGB')\n",
    "                image = preprocess(image)\n",
    "                images.append(image)\n",
    "                image_paths.append(path)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading image {path}: {e}\")\n",
    "    return image_paths, images\n",
    "\n",
    "gallery_folder = \"Data_example/test/gallery\"\n",
    "query_folder   = \"Data_example/test/query\"\n",
    "\n",
    "gallery_paths, gallery_images = load_images_from_folder(gallery_folder)\n",
    "query_paths, query_images     = load_images_from_folder(query_folder)\n",
    "\n",
    "# Extract embeddings\n",
    "def extract_embeddings(images):\n",
    "    embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for image in images:\n",
    "            image = image.unsqueeze(0).to(device)\n",
    "            outputs = model(image)\n",
    "            embedding = outputs.last_hidden_state[:, 0, :].squeeze().cpu().numpy()\n",
    "            embeddings.append(embedding)\n",
    "    return np.array(embeddings)\n",
    "\n",
    "gallery_embeddings = extract_embeddings(gallery_images)\n",
    "query_embeddings   = extract_embeddings(query_images)\n",
    "\n",
    "# Normalize\n",
    "gallery_embeddings = gallery_embeddings / np.linalg.norm(gallery_embeddings, axis=1, keepdims=True)\n",
    "query_embeddings   = query_embeddings / np.linalg.norm(query_embeddings, axis=1, keepdims=True)\n",
    "\n",
    "# FAISS index\n",
    "dimension = gallery_embeddings.shape[1]\n",
    "index = faiss.IndexFlatIP(dimension)\n",
    "index.add(gallery_embeddings)\n",
    "\n",
    "# Similarity search\n",
    "k = 5\n",
    "distances, indices = index.search(query_embeddings, k)\n",
    "\n",
    "# Prepare submission format\n",
    "results = []\n",
    "for i, query_path in enumerate(query_paths):\n",
    "    top_k_paths = [gallery_paths[idx] for idx in indices[i]]\n",
    "    results.append({\n",
    "        \"filename\": query_path,\n",
    "        \"gallery_images\": top_k_paths\n",
    "    })\n",
    "\n",
    "# Save to JSON\n",
    "with open(\"submission4.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(\"✅ Saved submission JSON to 'submission_vit_faiss.json'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d518b779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Triplet Network - modello 5\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models, transforms, datasets\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "# Triplet dataset\n",
    "class TripletImageDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.dataset = datasets.ImageFolder(root=root_dir)\n",
    "        self.classes = self.dataset.classes\n",
    "        self.class_to_idx = self.dataset.class_to_idx\n",
    "        self.transform = transform or transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "        # Build index for each class\n",
    "        self.class_index = {}\n",
    "        for idx, (path, label) in enumerate(self.dataset.samples):\n",
    "            if label not in self.class_index:\n",
    "                self.class_index[label] = []\n",
    "            self.class_index[label].append(path)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        anchor_path, anchor_label = self.dataset.samples[index]\n",
    "        positive_path = random.choice(self.class_index[anchor_label])\n",
    "\n",
    "        negative_label = random.choice([l for l in self.class_index if l != anchor_label])\n",
    "        negative_path = random.choice(self.class_index[negative_label])\n",
    "\n",
    "        anchor = self.transform(Image.open(anchor_path).convert('RGB'))\n",
    "        positive = self.transform(Image.open(positive_path).convert('RGB'))\n",
    "        negative = self.transform(Image.open(negative_path).convert('RGB'))\n",
    "\n",
    "        return anchor, positive, negative\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "# Triplet network using ResNet backbone\n",
    "class EmbeddingNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EmbeddingNet, self).__init__()\n",
    "        base_model = models.resnet18(pretrained=True)\n",
    "        modules = list(base_model.children())[:-1]  # Remove fc layer\n",
    "        self.feature_extractor = nn.Sequential(*modules)\n",
    "        self.fc = nn.Linear(base_model.fc.in_features, 128)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x).view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return F.normalize(x, p=2, dim=1)\n",
    "\n",
    "class TripletNet(nn.Module):\n",
    "    def __init__(self, embedding_net):\n",
    "        super(TripletNet, self).__init__()\n",
    "        self.embedding_net = embedding_net\n",
    "\n",
    "    def forward(self, anchor, positive, negative):\n",
    "        anchor_emb = self.embedding_net(anchor)\n",
    "        positive_emb = self.embedding_net(positive)\n",
    "        negative_emb = self.embedding_net(negative)\n",
    "        return anchor_emb, positive_emb, negative_emb\n",
    "\n",
    "# Training loop for TripletNet\n",
    "root_dir = 'Data_example/training'\n",
    "batch_size = 32\n",
    "num_epochs = 10\n",
    "learning_rate = 1e-4\n",
    "\n",
    "train_dataset = TripletImageDataset(root_dir)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "model = TripletNet(EmbeddingNet()).cuda()\n",
    "criterion = nn.TripletMarginLoss(margin=1.0)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "    for anchor, positive, negative in train_loader:\n",
    "        anchor, positive, negative = anchor.cuda(), positive.cuda(), negative.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        anchor_out, positive_out, negative_out = model(anchor, positive, negative)\n",
    "        loss = criterion(anchor_out, positive_out, negative_out)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "# Save trained model\n",
    "torch.save(model.embedding_net.state_dict(), 'triplet_embedding_model.pth')\n",
    "print(\"✅ Triplet model trained and saved as 'triplet_embedding_model.pth'\")\n",
    "\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Device (CPU only)\n",
    "device = torch.device('cpu')\n",
    "\n",
    "# Embedding network (must match the training definition)\n",
    "class EmbeddingNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EmbeddingNet, self).__init__()\n",
    "        base_model = models.resnet18(pretrained=False)\n",
    "        modules = list(base_model.children())[:-1]  # remove fc\n",
    "        self.feature_extractor = nn.Sequential(*modules)\n",
    "        self.fc = nn.Linear(base_model.fc.in_features, 128)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x).view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return F.normalize(x, p=2, dim=1)\n",
    "\n",
    "# Load trained model\n",
    "model = EmbeddingNet().to(device)\n",
    "model.load_state_dict(torch.load('triplet_embedding_model.pth', map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# Image transform (must match training)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Dataset class\n",
    "class ImageFolderDataset(Dataset):\n",
    "    def __init__(self, paths, transform):\n",
    "        self.paths = paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.paths[idx]\n",
    "        image = Image.open(path).convert('RGB')\n",
    "        return self.transform(image), path\n",
    "\n",
    "# Collect all image paths\n",
    "def get_image_paths(folder):\n",
    "    return [os.path.join(folder, f) for f in os.listdir(folder) if f.endswith('.jpg')]\n",
    "\n",
    "# Paths\n",
    "gallery_folder = 'Data_example/test/gallery'\n",
    "query_folder   = 'Data_example/test/query'\n",
    "\n",
    "gallery_paths = get_image_paths(gallery_folder)\n",
    "query_paths   = get_image_paths(query_folder)\n",
    "\n",
    "# Dataloaders\n",
    "batch_size = 32\n",
    "gallery_loader = DataLoader(ImageFolderDataset(gallery_paths, transform), batch_size=batch_size, shuffle=False)\n",
    "query_loader   = DataLoader(ImageFolderDataset(query_paths, transform),   batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Feature extraction\n",
    "@torch.no_grad()\n",
    "def extract_embeddings(loader):\n",
    "    feats, paths = [], []\n",
    "    for images, img_paths in loader:\n",
    "        images = images.to(device)\n",
    "        out = model(images)\n",
    "        feats.append(out.cpu().numpy())\n",
    "        paths.extend(img_paths)\n",
    "    return np.vstack(feats), paths\n",
    "\n",
    "# Extract features\n",
    "gallery_feats, gallery_paths = extract_embeddings(gallery_loader)\n",
    "query_feats, query_paths     = extract_embeddings(query_loader)\n",
    "\n",
    "# Normalize (just in case, though model already outputs normalized embeddings)\n",
    "gallery_feats = gallery_feats / np.linalg.norm(gallery_feats, axis=1, keepdims=True)\n",
    "query_feats   = query_feats / np.linalg.norm(query_feats,   axis=1, keepdims=True)\n",
    "\n",
    "# Retrieve top-k similar images\n",
    "k = 5\n",
    "results = []\n",
    "for q_feat, q_path in zip(query_feats, query_paths):\n",
    "    sims = gallery_feats @ q_feat  # cosine similarity\n",
    "    topk_idx = np.argsort(-sims)[:k]\n",
    "    topk_paths = [gallery_paths[i] for i in topk_idx]\n",
    "    results.append({\n",
    "        \"filename\": q_path,\n",
    "        \"gallery_images\": topk_paths\n",
    "    })\n",
    "\n",
    "# Save results\n",
    "with open('submission5.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(\"✅ Retrieval results saved to 'submission_triplet.json'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93961ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arcface - modello 6\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import models, transforms, datasets\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Paths\n",
    "train_dir = 'Data_example/training'\n",
    "gallery_dir = 'Data_example/test/gallery'\n",
    "query_dir = 'Data_example/test/query'\n",
    "\n",
    "# Parameters\n",
    "batch_size = 32\n",
    "embedding_dim = 128\n",
    "num_epochs = 10\n",
    "learning_rate = 1e-4\n",
    "k = 5  # top-k\n",
    "\n",
    "# Transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Dataset + Loader\n",
    "train_dataset = datasets.ImageFolder(train_dir, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "num_classes = len(train_dataset.classes)\n",
    "\n",
    "# ArcFace head\n",
    "class ArcFaceHead(nn.Module):\n",
    "    def __init__(self, in_features, out_features, s=30.0, m=0.50):\n",
    "        super(ArcFaceHead, self).__init__()\n",
    "        self.s = s\n",
    "        self.m = m\n",
    "        self.weight = nn.Parameter(torch.Tensor(out_features, in_features))\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "\n",
    "    def forward(self, input, label):\n",
    "        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n",
    "        theta = torch.acos(torch.clamp(cosine, -1.0 + 1e-7, 1.0 - 1e-7))\n",
    "        target_logit = torch.cos(theta + self.m)\n",
    "        one_hot = torch.zeros_like(cosine)\n",
    "        one_hot.scatter_(1, label.view(-1, 1), 1.0)\n",
    "        output = cosine * (1 - one_hot) + target_logit * one_hot\n",
    "        return self.s * output\n",
    "\n",
    "# EmbeddingNet\n",
    "class EmbeddingNet(nn.Module):\n",
    "    def __init__(self, out_dim=embedding_dim):\n",
    "        super(EmbeddingNet, self).__init__()\n",
    "        base_model = models.resnet18(pretrained=True)\n",
    "        self.backbone = nn.Sequential(*list(base_model.children())[:-1])\n",
    "        self.embedding = nn.Linear(base_model.fc.in_features, out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x).view(x.size(0), -1)\n",
    "        return F.normalize(self.embedding(x))\n",
    "\n",
    "# Model + Loss + Optimizer\n",
    "model = EmbeddingNet().to(device)\n",
    "arcface_head = ArcFaceHead(embedding_dim, num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(list(model.parameters()) + list(arcface_head.parameters()), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "best_loss = float('inf')\n",
    "best_model_state = None\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        embeddings = model(images)\n",
    "        logits = arcface_head(embeddings, labels)\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Save best model\n",
    "    if avg_loss < best_loss:\n",
    "        best_loss = avg_loss\n",
    "        best_model_state = model.state_dict().copy()\n",
    "\n",
    "# Save best model\n",
    "torch.save(best_model_state, 'arcface_embedding_model.pth')\n",
    "\n",
    "# Dataset for retrieval\n",
    "class ImageFolderPaths(Dataset):\n",
    "    def __init__(self, paths, transform):\n",
    "        self.paths = paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.paths[idx]\n",
    "        image = Image.open(path).convert(\"RGB\")\n",
    "        return self.transform(image), path\n",
    "\n",
    "def get_image_paths(folder):\n",
    "    return [os.path.join(folder, f) for f in os.listdir(folder) if f.endswith(\".jpg\")]\n",
    "\n",
    "def extract_embeddings(model, loader):\n",
    "    model.eval()\n",
    "    features, paths = [], []\n",
    "    with torch.no_grad():\n",
    "        for imgs, pths in loader:\n",
    "            imgs = imgs.to(device)\n",
    "            out = model(imgs).cpu().numpy()\n",
    "            features.append(out)\n",
    "            paths.extend(pths)\n",
    "    return np.vstack(features), paths\n",
    "\n",
    "# Reload model\n",
    "model = EmbeddingNet().to(device)\n",
    "model.load_state_dict(torch.load('arcface_embedding_model.pth', map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# Extract gallery/query embeddings\n",
    "gallery_paths = get_image_paths(gallery_dir)\n",
    "query_paths = get_image_paths(query_dir)\n",
    "\n",
    "gallery_loader = DataLoader(ImageFolderPaths(gallery_paths, transform), batch_size=batch_size, shuffle=False)\n",
    "query_loader = DataLoader(ImageFolderPaths(query_paths, transform), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "gallery_feats, gallery_paths = extract_embeddings(model, gallery_loader)\n",
    "query_feats, query_paths = extract_embeddings(model, query_loader)\n",
    "\n",
    "# Normalize embeddings\n",
    "gallery_feats = normalize(gallery_feats, axis=1)\n",
    "query_feats = normalize(query_feats, axis=1)\n",
    "\n",
    "# Compute cosine similarities and save submission\n",
    "results = []\n",
    "for q_feat, q_path in zip(query_feats, query_paths):\n",
    "    sims = gallery_feats @ q_feat\n",
    "    topk_idx = np.argsort(-sims)[:k]\n",
    "    topk_paths = [gallery_paths[i] for i in topk_idx]\n",
    "    results.append({\n",
    "        'filename': q_path,\n",
    "        'gallery_images': topk_paths\n",
    "    })\n",
    "\n",
    "with open('submission6.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(\"✅ Saved submission_arcface.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0eeadf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clip fine-tuned - modello 7\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Parameters\n",
    "train_dir = 'Data_example/training'\n",
    "gallery_dir = 'Data_example/test/gallery'\n",
    "query_dir = 'Data_example/test/query'\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "learning_rate = 1e-5\n",
    "top_k = 5\n",
    "\n",
    "# Load CLIP\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "# Custom dataset for image-text pairs\n",
    "class ImageTextDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.dataset = datasets.ImageFolder(root=root_dir)\n",
    "        self.label_to_text = {v: k for k, v in self.dataset.class_to_idx.items()}\n",
    "        self.transform = transform or transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.48145466, 0.4578275, 0.40821073],\n",
    "                                 [0.26862954, 0.26130258, 0.27577711])\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self.dataset[idx]\n",
    "        text = self.label_to_text[label].replace('_', ' ')\n",
    "        image = self.transform(image)\n",
    "        return image, text\n",
    "\n",
    "train_dataset = ImageTextDataset(train_dir)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Fine-tuning loop\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for images, texts in train_loader:\n",
    "        inputs = processor(text=texts, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "        inputs[\"pixel_values\"] = images.to(device)\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        logits_per_image = outputs.logits_per_image\n",
    "        labels = torch.arange(len(images), device=device)\n",
    "        loss = nn.CrossEntropyLoss()(logits_per_image, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "# Save fine-tuned model\n",
    "model.save_pretrained(\"clip_finetuned\")\n",
    "processor.save_pretrained(\"clip_finetuned\")\n",
    "\n",
    "# Dataset for inference\n",
    "class ImageFolderPaths(Dataset):\n",
    "    def __init__(self, paths, transform=None):\n",
    "        self.paths = paths\n",
    "        self.transform = transform or transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.48145466, 0.4578275, 0.40821073],\n",
    "                                 [0.26862954, 0.26130258, 0.27577711])\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.paths[idx]\n",
    "        image = Image.open(path).convert(\"RGB\")\n",
    "        return self.transform(image), path\n",
    "\n",
    "def get_image_paths(folder):\n",
    "    return [os.path.join(folder, f) for f in os.listdir(folder) if f.endswith(\".jpg\")]\n",
    "\n",
    "def extract_image_embeddings(paths):\n",
    "    dataset = ImageFolderPaths(paths)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    features, all_paths = [], []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, img_paths in loader:\n",
    "            inputs = {\"pixel_values\": images.to(device)}\n",
    "            image_features = model.get_image_features(**inputs)\n",
    "            image_features = image_features.cpu().numpy()\n",
    "            features.append(image_features)\n",
    "            all_paths.extend(img_paths)\n",
    "    return np.vstack(features), all_paths\n",
    "\n",
    "# Load fine-tuned model for inference\n",
    "model = CLIPModel.from_pretrained(\"clip_finetuned\").to(device)\n",
    "processor = CLIPProcessor.from_pretrained(\"clip_finetuned\")\n",
    "\n",
    "# Extract gallery and query features\n",
    "gallery_paths = get_image_paths(gallery_dir)\n",
    "query_paths = get_image_paths(query_dir)\n",
    "\n",
    "gallery_feats, gallery_paths = extract_image_embeddings(gallery_paths)\n",
    "query_feats, query_paths = extract_image_embeddings(query_paths)\n",
    "\n",
    "# Normalize features\n",
    "gallery_feats = normalize(gallery_feats, axis=1)\n",
    "query_feats = normalize(query_feats, axis=1)\n",
    "\n",
    "# Similarity and submission\n",
    "results = []\n",
    "for q_feat, q_path in zip(query_feats, query_paths):\n",
    "    sims = gallery_feats @ q_feat\n",
    "    topk_idx = np.argsort(-sims)[:top_k]\n",
    "    topk_paths = [gallery_paths[i] for i in topk_idx]\n",
    "    results.append({\n",
    "        \"filename\": q_path,\n",
    "        \"gallery_images\": topk_paths\n",
    "    })\n",
    "\n",
    "with open(\"submission7.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(\"✅ Saved submission_clip.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb28ee7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SimCLR - modello 8\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, models, datasets\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# Configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_dir = 'Data_example/training'\n",
    "gallery_dir = 'Data_example/test/gallery'\n",
    "query_dir = 'Data_example/test/query'\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "learning_rate = 1e-3\n",
    "embedding_dim = 128\n",
    "top_k = 5\n",
    "\n",
    "# Augmentation for contrastive learning\n",
    "simclr_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(0.8, 0.8, 0.8, 0.2),\n",
    "    transforms.RandomGrayscale(p=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# SimCLR Dataset\n",
    "class SimCLRDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform):\n",
    "        self.dataset = datasets.ImageFolder(root=root_dir)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, _ = self.dataset[idx]\n",
    "        return self.transform(img), self.transform(img)\n",
    "\n",
    "# Projection head\n",
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, in_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_dim, out_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# SimCLR model\n",
    "class SimCLR(nn.Module):\n",
    "    def __init__(self, base_model, projection_dim):\n",
    "        super().__init__()\n",
    "        self.backbone = nn.Sequential(*list(base_model.children())[:-1])\n",
    "        self.projection_head = ProjectionHead(base_model.fc.in_features, projection_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x).squeeze()\n",
    "        x = self.projection_head(x)\n",
    "        return F.normalize(x, dim=-1)\n",
    "\n",
    "# NT-Xent Loss\n",
    "def nt_xent_loss(z1, z2, temperature=0.5):\n",
    "    N = z1.size(0)\n",
    "    z = torch.cat([z1, z2], dim=0)\n",
    "    sim = F.cosine_similarity(z.unsqueeze(1), z.unsqueeze(0), dim=2)\n",
    "    sim = sim / temperature\n",
    "\n",
    "    labels = torch.arange(N, device=device)\n",
    "    labels = torch.cat([labels + N, labels])\n",
    "\n",
    "    mask = torch.eye(2 * N, dtype=torch.bool).to(device)\n",
    "    sim = sim.masked_fill(mask, -9e15)\n",
    "\n",
    "    targets = labels\n",
    "    loss = F.cross_entropy(sim, targets)\n",
    "    return loss\n",
    "\n",
    "# Dataset and model\n",
    "train_dataset = SimCLRDataset(train_dir, simclr_transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "resnet = models.resnet18(pretrained=True)\n",
    "model = SimCLR(resnet, embedding_dim).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for x1, x2 in train_loader:\n",
    "        x1, x2 = x1.to(device), x2.to(device)\n",
    "        z1 = model(x1)\n",
    "        z2 = model(x2)\n",
    "        loss = nt_xent_loss(z1, z2)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "# Save model\n",
    "torch.save(model.state_dict(), \"simclr_embedding_model.pth\")\n",
    "\n",
    "# Inference dataset\n",
    "class ImageFolderPaths(Dataset):\n",
    "    def __init__(self, paths, transform):\n",
    "        self.paths = paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.paths[idx]\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        return self.transform(img), path\n",
    "\n",
    "# Simple transform for test\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "def get_image_paths(folder):\n",
    "    return [os.path.join(folder, f) for f in os.listdir(folder) if f.endswith('.jpg')]\n",
    "\n",
    "def extract_embeddings(model, image_paths):\n",
    "    dataset = ImageFolderPaths(image_paths, test_transform)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size)\n",
    "    model.eval()\n",
    "    features, paths = [], []\n",
    "    with torch.no_grad():\n",
    "        for images, pths in loader:\n",
    "            images = images.to(device)\n",
    "            emb = model(images).cpu().numpy()\n",
    "            features.append(emb)\n",
    "            paths.extend(pths)\n",
    "    return np.vstack(features), paths\n",
    "\n",
    "# Load model for inference\n",
    "model = SimCLR(resnet, embedding_dim).to(device)\n",
    "model.load_state_dict(torch.load(\"simclr_embedding_model.pth\", map_location=device))\n",
    "model.eval()\n",
    "\n",
    "gallery_paths = get_image_paths(gallery_dir)\n",
    "query_paths = get_image_paths(query_dir)\n",
    "\n",
    "gallery_feats, gallery_paths = extract_embeddings(model, gallery_paths)\n",
    "query_feats, query_paths = extract_embeddings(model, query_paths)\n",
    "\n",
    "gallery_feats = normalize(gallery_feats, axis=1)\n",
    "query_feats = normalize(query_feats, axis=1)\n",
    "\n",
    "# Similarity + JSON output\n",
    "results = []\n",
    "for q_feat, q_path in zip(query_feats, query_paths):\n",
    "    sims = gallery_feats @ q_feat\n",
    "    topk_idx = np.argsort(-sims)[:top_k]\n",
    "    topk_paths = [gallery_paths[i] for i in topk_idx]\n",
    "    results.append({\n",
    "        \"filename\": q_path,\n",
    "        \"gallery_images\": topk_paths\n",
    "    })\n",
    "\n",
    "with open(\"submission8.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(\"✅ Saved submission_simclr.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75afae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FAISS - modello 9 \n",
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Configurazione\n",
    "gallery_folder = 'Data_example/test/gallery'\n",
    "query_folder   = 'Data_example/test/query'\n",
    "top_k = 3\n",
    "\n",
    "# Carica il modello pre-addestrato\n",
    "model = models.resnet50(pretrained=True)\n",
    "model = torch.nn.Sequential(*list(model.children())[:-1])  # Rimuove l'ultima classificazione\n",
    "model.eval()\n",
    "\n",
    "# Trasformazioni per le immagini\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Funzione per estrarre le feature\n",
    "def extract_features(image_path):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image = transform(image).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        feature = model(image)\n",
    "    return feature.squeeze().numpy()\n",
    "\n",
    "# Estrai feature per la galleria\n",
    "gallery_paths = [os.path.join(gallery_folder, fname) for fname in os.listdir(gallery_folder)]\n",
    "gallery_features = np.array([extract_features(p) for p in gallery_paths]).astype('float32')\n",
    "\n",
    "# Costruisci l’indice FAISS\n",
    "index = faiss.IndexFlatL2(gallery_features.shape[1])\n",
    "index.add(gallery_features)\n",
    "\n",
    "# Estrai feature per ogni immagine query e trova le immagini più simili\n",
    "results = []\n",
    "query_paths = [os.path.join(query_folder, fname) for fname in os.listdir(query_folder)]\n",
    "\n",
    "for q_path in query_paths:\n",
    "    q_feat = extract_features(q_path).astype('float32').reshape(1, -1)\n",
    "    distances, indices = index.search(q_feat, top_k)\n",
    "    similar_images = [gallery_paths[i] for i in indices[0]]\n",
    "    results.append({\n",
    "        \"filename\": q_path,\n",
    "        \"gallery_images\": similar_images\n",
    "    })\n",
    "\n",
    "\n",
    "# Salva o stampa il risultato in formato JSON\n",
    "with open('submission2.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print('submission ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e25991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Siamese fine-tuned- modello 10\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# ================================\n",
    "# CONFIG\n",
    "# ================================\n",
    "training_dir = 'Data_example/training'\n",
    "batch_size = 8\n",
    "num_epochs = 15\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ================================\n",
    "# DATASET: Siamese con coppie auto-generate\n",
    "# ================================\n",
    "class SiameseDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.transform = transform or transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "        self.root_dir = root_dir\n",
    "\n",
    "        self.class_to_imgs = {}\n",
    "        for cls in os.listdir(root_dir):\n",
    "            class_path = os.path.join(root_dir, cls)\n",
    "            if os.path.isdir(class_path):\n",
    "                self.class_to_imgs[cls] = [\n",
    "                    os.path.join(class_path, f) for f in os.listdir(class_path)\n",
    "                    if f.lower().endswith(('.jpg', '.jpeg', '.png'))\n",
    "                ]\n",
    "\n",
    "        self.classes = list(self.class_to_imgs.keys())\n",
    "        self.pairs = self._generate_pairs()\n",
    "\n",
    "    def _generate_pairs(self, num_pairs=1000):\n",
    "        pairs = []\n",
    "\n",
    "        for _ in range(num_pairs):\n",
    "            # Positiva\n",
    "            cls = random.choice(self.classes)\n",
    "            imgs = random.sample(self.class_to_imgs[cls], 2)\n",
    "            pairs.append((imgs[0], imgs[1], 1))\n",
    "\n",
    "            # Negativa\n",
    "            cls1, cls2 = random.sample(self.classes, 2)\n",
    "            img1 = random.choice(self.class_to_imgs[cls1])\n",
    "            img2 = random.choice(self.class_to_imgs[cls2])\n",
    "            pairs.append((img1, img2, 0))\n",
    "\n",
    "        return pairs\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img1_path, img2_path, label = self.pairs[index]\n",
    "        img1 = Image.open(img1_path).convert('RGB')\n",
    "        img2 = Image.open(img2_path).convert('RGB')\n",
    "        return self.transform(img1), self.transform(img2), torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "# ================================\n",
    "# MODEL: Siamese Network\n",
    "# ================================\n",
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        self.backbone = models.resnet18(pretrained=True)\n",
    "        self.backbone.fc = nn.Identity()  # Rimuove classificazione finale\n",
    "        self.embedding_dim = 512\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.embedding_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128)\n",
    "        )\n",
    "\n",
    "    def forward_once(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        return self.forward_once(x1), self.forward_once(x2)\n",
    "\n",
    "# ================================\n",
    "# LOSS: Contrastive\n",
    "# ================================\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, out1, out2, label):\n",
    "        euclidean = nn.functional.pairwise_distance(out1, out2)\n",
    "        loss = torch.mean((1 - label) * torch.pow(euclidean, 2) +\n",
    "                          label * torch.pow(torch.clamp(self.margin - euclidean, min=0.0), 2))\n",
    "        return loss\n",
    "\n",
    "# ================================\n",
    "# TRAINING\n",
    "# ================================\n",
    "dataset = SiameseDataset(training_dir)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "model = SiameseNetwork().to(device)\n",
    "criterion = ContrastiveLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "print(\"🚀 Inizio training...\")\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for img1, img2, labels in dataloader:\n",
    "        img1, img2, labels = img1.to(device), img2.to(device), labels.to(device)\n",
    "\n",
    "        out1, out2 = model(img1, img2)\n",
    "        loss = criterion(out1, out2, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | Loss: {total_loss / len(dataloader):.4f}\")\n",
    "print(\"✅ Fine allenamento.\")\n",
    "\n",
    "\n",
    "# Cartelle\n",
    "gallery_dir = 'Data_example/test/gallery'\n",
    "query_dir   = 'Data_example/test/query'\n",
    "\n",
    "# Trasformazioni da usare anche qui\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "def extract_embedding(image_path, model):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image = transform(image).unsqueeze(0).to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        embedding = model.forward_once(image)\n",
    "    return embedding.cpu().numpy().flatten()\n",
    "\n",
    "# Estrai feature galleria\n",
    "gallery_paths = [os.path.join(gallery_dir, f) for f in os.listdir(gallery_dir)]\n",
    "gallery_embeddings = [extract_embedding(p, model) for p in gallery_paths]\n",
    "\n",
    "# Estrai feature query\n",
    "query_paths = [os.path.join(query_dir, f) for f in os.listdir(query_dir)]\n",
    "\n",
    "results = []\n",
    "top_k = 3  # Numero di immagini simili da restituire\n",
    "\n",
    "for q_path in query_paths:\n",
    "    q_emb = extract_embedding(q_path, model).reshape(1, -1)\n",
    "    sims = cosine_similarity(q_emb, gallery_embeddings)[0]\n",
    "    top_indices = sims.argsort()[::-1][:top_k]\n",
    "    similar_images = [gallery_paths[i] for i in top_indices]\n",
    "    results.append({\n",
    "        \"filename\": q_path,\n",
    "        \"gallery_images\": similar_images\n",
    "    })\n",
    "\n",
    "# Salva in JSON\n",
    "     # Salva o stampa il risultato in formato JSON\n",
    "    with open('submission3.json', 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "\n",
    "    print('submission ok')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7ac5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Siamese double fine-tuned - modello 11\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import json\n",
    "\n",
    "# ================================\n",
    "# CONFIGURAZIONE\n",
    "# ================================\n",
    "class Config:\n",
    "    # Directory paths\n",
    "    gallery_dir = 'Data_example/test/gallery'\n",
    "    query_dir = 'Data_example/test/query'\n",
    "    model_path = 'best_siamese_model.pth'\n",
    "    output_json = 'submission7.json'\n",
    "    \n",
    "    # Training parameters - MODIFICA QUI I PARAMETRI DI TRAINING\n",
    "    batch_size = 16\n",
    "    num_epochs = 5          # ← CAMBIA QUI IL NUMERO DI EPOCH\n",
    "    learning_rate = 1e-4    # ← CAMBIA QUI IL LEARNING RATE\n",
    "    embedding_dim = 256     # ← CAMBIA QUI LA DIMENSIONE DELL'EMBEDDING\n",
    "    margin = 2.0           # ← CAMBIA QUI IL MARGIN PER LA CONTRASTIVE LOSS\n",
    "    num_pairs = 2000       # ← CAMBIA QUI IL NUMERO DI COPPIE PER TRAINING\n",
    "    top_k = 5              # ← CAMBIA QUI IL NUMERO DI RISULTATI TOP-K\n",
    "    \n",
    "    # Model parameters\n",
    "    dropout_rate = 0.3     # ← CAMBIA QUI IL DROPOUT RATE\n",
    "    hidden_dim = 512       # ← CAMBIA QUI LA DIMENSIONE HIDDEN LAYER\n",
    "    \n",
    "    # Device configuration\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Data augmentation and preprocessing\n",
    "    use_data_augmentation = True  # ← ATTIVA/DISATTIVA DATA AUGMENTATION\n",
    "    \n",
    "    # Trasformazioni per training (con data augmentation)\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.RandomCrop((224, 224)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomRotation(degrees=15),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # Trasformazioni per test (senza augmentation)\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    @classmethod\n",
    "    def print_config(cls):\n",
    "        \"\"\"Stampa la configurazione corrente\"\"\"\n",
    "        print(\"🔧 CONFIGURAZIONE CORRENTE\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"📁 Gallery directory: {cls.gallery_dir}\")\n",
    "        print(f\"📁 Query directory: {cls.query_dir}\")\n",
    "        print(f\"💾 Model path: {cls.model_path}\")\n",
    "        print(f\"📄 Output JSON: {cls.output_json}\")\n",
    "        print(\"\\n🎯 PARAMETRI DI TRAINING:\")\n",
    "        print(f\"   Batch size: {cls.batch_size}\")\n",
    "        print(f\"   Num epochs: {cls.num_epochs}\")\n",
    "        print(f\"   Learning rate: {cls.learning_rate}\")\n",
    "        print(f\"   Embedding dim: {cls.embedding_dim}\")\n",
    "        print(f\"   Margin: {cls.margin}\")\n",
    "        print(f\"   Num pairs: {cls.num_pairs}\")\n",
    "        print(f\"   Top-K: {cls.top_k}\")\n",
    "        print(f\"   Dropout rate: {cls.dropout_rate}\")\n",
    "        print(f\"   Hidden dim: {cls.hidden_dim}\")\n",
    "        print(f\"   Data augmentation: {cls.use_data_augmentation}\")\n",
    "        print(f\"   Device: {cls.device}\")\n",
    "        print(\"=\" * 50)\n",
    "    \n",
    "    @classmethod\n",
    "    def update_training_params(cls, **kwargs):\n",
    "        \"\"\"Aggiorna i parametri di training dinamicamente\"\"\"\n",
    "        updated = []\n",
    "        for key, value in kwargs.items():\n",
    "            if hasattr(cls, key):\n",
    "                old_value = getattr(cls, key)\n",
    "                setattr(cls, key, value)\n",
    "                updated.append(f\"{key}: {old_value} → {value}\")\n",
    "            else:\n",
    "                print(f\"⚠️  Parametro sconosciuto: {key}\")\n",
    "        \n",
    "        if updated:\n",
    "            print(\"✅ Parametri aggiornati:\")\n",
    "            for update in updated:\n",
    "                print(f\"   {update}\")\n",
    "\n",
    "# ================================\n",
    "# MODELLO SIAMESE\n",
    "# ================================\n",
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self, embedding_dim=None, dropout_rate=None, hidden_dim=None):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        \n",
    "        # Usa i parametri dalla Config se non specificati\n",
    "        if embedding_dim is None:\n",
    "            embedding_dim = Config.embedding_dim\n",
    "        if dropout_rate is None:\n",
    "            dropout_rate = Config.dropout_rate\n",
    "        if hidden_dim is None:\n",
    "            hidden_dim = Config.hidden_dim\n",
    "        \n",
    "        # Backbone: ResNet18 pre-addestrato\n",
    "        self.backbone = models.resnet18(pretrained=True)\n",
    "        \n",
    "        # Rimuovi l'ultimo layer di classificazione\n",
    "        num_features = self.backbone.fc.in_features\n",
    "        self.backbone.fc = nn.Identity()\n",
    "        \n",
    "        # Embedding layers con parametri configurabili\n",
    "        self.embedding = nn.Sequential(\n",
    "            nn.Linear(num_features, hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_dim, embedding_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout_rate * 0.7),  # Dropout più leggero nel secondo layer\n",
    "            nn.Linear(embedding_dim, embedding_dim)\n",
    "        )\n",
    "        \n",
    "        print(f\"🏗️  Modello creato con:\")\n",
    "        print(f\"   Embedding dim: {embedding_dim}\")\n",
    "        print(f\"   Hidden dim: {hidden_dim}\")\n",
    "        print(f\"   Dropout rate: {dropout_rate}\")\n",
    "    \n",
    "    def forward_once(self, x):\n",
    "        features = self.backbone(x)\n",
    "        embedding = self.embedding(features)\n",
    "        # Normalizza l'embedding per stabilità\n",
    "        embedding = F.normalize(embedding, p=2, dim=1)\n",
    "        return embedding\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        emb1 = self.forward_once(x1)\n",
    "        emb2 = self.forward_once(x2)\n",
    "        return emb1, emb2\n",
    "\n",
    "# ================================\n",
    "# CONTRASTIVE LOSS\n",
    "# ================================\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin=None):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin if margin is not None else Config.margin\n",
    "        print(f\"📊 Contrastive Loss creata con margin: {self.margin}\")\n",
    "    \n",
    "    def forward(self, output1, output2, label):\n",
    "        euclidean_distance = F.pairwise_distance(output1, output2, keepdim=True)\n",
    "        loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +\n",
    "                                    (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n",
    "        return loss_contrastive\n",
    "\n",
    "# ================================\n",
    "# FUNZIONI DI UTILITÀ\n",
    "# ================================\n",
    "def load_model():\n",
    "    \"\"\"Carica il modello addestrato con parametri dalla Config\"\"\"\n",
    "    model = SiameseNetwork().to(Config.device)\n",
    "    \n",
    "    if os.path.exists(Config.model_path):\n",
    "        model.load_state_dict(torch.load(Config.model_path, map_location=Config.device))\n",
    "        print(f\"✅ Modello caricato da {Config.model_path}\")\n",
    "    else:\n",
    "        print(f\"⚠️  Modello non trovato in {Config.model_path}. Usando modello non addestrato.\")\n",
    "    \n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def extract_embedding(image_path, model):\n",
    "    \"\"\"Estrae embedding da una singola immagine\"\"\"\n",
    "    try:\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        image = Config.test_transform(image).unsqueeze(0).to(Config.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            embedding = model.forward_once(image)\n",
    "        \n",
    "        return embedding.cpu().numpy().flatten()\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Errore nell'estrazione embedding da {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_image_paths(directory):\n",
    "    \"\"\"Ottiene tutti i percorsi delle immagini in una directory\"\"\"\n",
    "    if not os.path.exists(directory):\n",
    "        print(f\"❌ Directory non trovata: {directory}\")\n",
    "        return []\n",
    "    \n",
    "    valid_extensions = ('.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.webp')\n",
    "    image_paths = []\n",
    "    \n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.lower().endswith(valid_extensions):\n",
    "            image_paths.append(os.path.join(directory, filename))\n",
    "    \n",
    "    return sorted(image_paths)\n",
    "\n",
    "def generate_similarity_json():\n",
    "    \"\"\"Genera il file JSON con i risultati di similarità\"\"\"\n",
    "    print(\"🔍 Generazione JSON per similarità immagini...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Stampa configurazione corrente\n",
    "    Config.print_config()\n",
    "    \n",
    "    # Carica il modello\n",
    "    model = load_model()\n",
    "    \n",
    "    # Ottieni percorsi immagini\n",
    "    gallery_paths = get_image_paths(Config.gallery_dir)\n",
    "    query_paths = get_image_paths(Config.query_dir)\n",
    "    \n",
    "    if not gallery_paths:\n",
    "        print(f\"❌ Nessuna immagine trovata in {Config.gallery_dir}\")\n",
    "        return\n",
    "    \n",
    "    if not query_paths:\n",
    "        print(f\"❌ Nessuna immagine trovata in {Config.query_dir}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"📁 Gallery: {len(gallery_paths)} immagini\")\n",
    "    print(f\"📁 Query: {len(query_paths)} immagini\")\n",
    "    \n",
    "    # Estrai embedding per tutte le immagini gallery\n",
    "    print(\"\\n🧮 Estrazione embedding gallery...\")\n",
    "    gallery_embeddings = []\n",
    "    valid_gallery_paths = []\n",
    "    \n",
    "    for i, path in enumerate(gallery_paths):\n",
    "        if i % 50 == 0:\n",
    "            print(f\"Processando gallery {i+1}/{len(gallery_paths)}\")\n",
    "        \n",
    "        emb = extract_embedding(path, model)\n",
    "        if emb is not None:\n",
    "            gallery_embeddings.append(emb)\n",
    "            valid_gallery_paths.append(path)\n",
    "    \n",
    "    if not gallery_embeddings:\n",
    "        print(\"❌ Nessun embedding valido estratto dalla gallery\")\n",
    "        return\n",
    "    \n",
    "    gallery_embeddings = np.array(gallery_embeddings)\n",
    "    print(f\"✅ Estratti {len(gallery_embeddings)} embedding dalla gallery\")\n",
    "    \n",
    "    # Processa ogni query e trova le immagini più simili\n",
    "    print(f\"\\n🔍 Ricerca top-{Config.top_k} immagini simili per ogni query...\")\n",
    "    results = []\n",
    "    \n",
    "    for i, query_path in enumerate(query_paths):\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Processando query {i+1}/{len(query_paths)}\")\n",
    "        \n",
    "        # Estrai embedding della query\n",
    "        query_emb = extract_embedding(query_path, model)\n",
    "        if query_emb is None:\n",
    "            print(f\"⚠️  Saltando {query_path} (errore estrazione embedding)\")\n",
    "            continue\n",
    "        \n",
    "        # Calcola similarità con tutte le immagini gallery\n",
    "        query_emb = query_emb.reshape(1, -1)\n",
    "        similarities = cosine_similarity(query_emb, gallery_embeddings)[0]\n",
    "        \n",
    "        # Trova le top-k immagini più simili\n",
    "        top_indices = similarities.argsort()[::-1][:Config.top_k]\n",
    "        similar_images = [valid_gallery_paths[idx] for idx in top_indices]\n",
    "        \n",
    "        # Formato semplice richiesto: filename + gallery_images\n",
    "        result_entry = {\n",
    "            \"filename\": query_path,\n",
    "            \"gallery_images\": similar_images\n",
    "        }\n",
    "        results.append(result_entry)\n",
    "        \n",
    "        # Debug: mostra similarità per le prime query\n",
    "        if i < 3:\n",
    "            print(f\"\\n📊 Query: {os.path.basename(query_path)}\")\n",
    "            for j, idx in enumerate(top_indices):\n",
    "                sim_score = similarities[idx]\n",
    "                img_name = os.path.basename(valid_gallery_paths[idx])\n",
    "                print(f\"  {j+1}. {img_name} (similarità: {sim_score:.4f})\")\n",
    "            print(f\"  → Formato finale: filename + {len(similar_images)} gallery_images\")\n",
    "    \n",
    "    # Salva risultati in JSON\n",
    "    print(f\"\\n💾 Salvataggio risultati in {Config.output_json}...\")\n",
    "    with open(Config.output_json, 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    print(f\"✅ JSON generato con successo!\")\n",
    "    print(f\"📄 File: {Config.output_json}\")\n",
    "    print(f\"📊 Query processate: {len(results)}\")\n",
    "    print(f\"🎯 Top-{Config.top_k} immagini simili per ogni query\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def validate_json_format():\n",
    "    \"\"\"Valida il formato del JSON generato\"\"\"\n",
    "    if not os.path.exists(Config.output_json):\n",
    "        print(f\"❌ File {Config.output_json} non trovato\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        with open(Config.output_json, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        print(f\"\\n🔍 Validazione formato JSON...\")\n",
    "        print(f\"📊 Numero di entries: {len(data)}\")\n",
    "        \n",
    "        # Controlla il formato della prima entry\n",
    "        if data:\n",
    "            first_entry = data[0]\n",
    "            required_keys = ['filename', 'gallery_images']\n",
    "            \n",
    "            for key in required_keys:\n",
    "                if key not in first_entry:\n",
    "                    print(f\"❌ Chiave mancante: {key}\")\n",
    "                    return False\n",
    "            \n",
    "            print(f\"✅ Formato corretto!\")\n",
    "            print(f\"📝 Esempio prima entry:\")\n",
    "            print(f\"   filename: {os.path.basename(first_entry['filename'])}\")\n",
    "            print(f\"   gallery_images: {len(first_entry['gallery_images'])} immagini\")\n",
    "            print(f\"   📄 Struttura: [{{\\\"filename\\\": \\\"...\\\", \\\"gallery_images\\\": [...]}}]\")\n",
    "            \n",
    "            return True\n",
    "        else:\n",
    "            print(\"❌ JSON vuoto\")\n",
    "            return False\n",
    "            \n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"❌ Errore formato JSON: {e}\")\n",
    "        return False\n",
    "\n",
    "# ================================\n",
    "# FUNZIONI PER MODIFICARE PARAMETRI\n",
    "# ================================\n",
    "def quick_config_update(**kwargs):\n",
    "    \"\"\"Funzione rapida per aggiornare la configurazione\"\"\"\n",
    "    print(\"🔧 Aggiornamento configurazione...\")\n",
    "    Config.update_training_params(**kwargs)\n",
    "    print(\"\\n📋 Configurazione aggiornata:\")\n",
    "    Config.print_config()\n",
    "\n",
    "def training_preset_light():\n",
    "    \"\"\"Preset per training leggero e veloce\"\"\"\n",
    "    Config.update_training_params(\n",
    "        batch_size=32,\n",
    "        num_epochs=3,\n",
    "        learning_rate=1e-3,\n",
    "        embedding_dim=128,\n",
    "        margin=1.5,\n",
    "        num_pairs=1000,\n",
    "        dropout_rate=0.2\n",
    "    )\n",
    "    print(\"🚀 Preset LIGHT applicato - Training veloce\")\n",
    "\n",
    "def training_preset_heavy():\n",
    "    \"\"\"Preset per training intensivo e accurato\"\"\"\n",
    "    Config.update_training_params(\n",
    "        batch_size=8,\n",
    "        num_epochs=15,\n",
    "        learning_rate=5e-5,\n",
    "        embedding_dim=512,\n",
    "        margin=3.0,\n",
    "        num_pairs=5000,\n",
    "        dropout_rate=0.4\n",
    "    )\n",
    "    print(\"🔥 Preset HEAVY applicato - Training intensivo\")\n",
    "\n",
    "def training_preset_balanced():\n",
    "    \"\"\"Preset bilanciato tra velocità e accuratezza\"\"\"\n",
    "    Config.update_training_params(\n",
    "        batch_size=16,\n",
    "        num_epochs=8,\n",
    "        learning_rate=1e-4,\n",
    "        embedding_dim=256,\n",
    "        margin=2.0,\n",
    "        num_pairs=3000,\n",
    "        dropout_rate=0.3\n",
    "    )\n",
    "    print(\"⚖️  Preset BALANCED applicato - Training bilanciato\")\n",
    "\n",
    "# ================================\n",
    "# MAIN\n",
    "# ================================\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"🎯 Generatore JSON per Similarità Immagini\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Esempi di come modificare i parametri:\n",
    "    \n",
    "    # 1. Modifica singoli parametri\n",
    "    # Config.update_training_params(num_epochs=10, learning_rate=5e-5)\n",
    "    \n",
    "    # 2. Modifica multipli parametri\n",
    "    # quick_config_update(batch_size=32, num_epochs=8, embedding_dim=512)\n",
    "    \n",
    "    # 3. Usa preset predefiniti\n",
    "    # training_preset_light()    # Per training veloce\n",
    "    # training_preset_heavy()    # Per training intensivo  \n",
    "    # training_preset_balanced() # Per training bilanciato\n",
    "    \n",
    "    # Genera il JSON\n",
    "    results = generate_similarity_json()\n",
    "    \n",
    "    if results:\n",
    "        # Valida il formato\n",
    "        validate_json_format()\n",
    "        \n",
    "        print(\"\\n🎉 Processo completato!\")\n",
    "        print(f\"📁 File generato: {Config.output_json}\")\n",
    "    else:\n",
    "        print(\"❌ Errore nella generazione del JSON\")\n",
    "\n",
    "# ================================\n",
    "# FUNZIONE PER GENERARE DATI MOCK (se non hai il modello)\n",
    "# ================================\n",
    "def generate_mock_json():\n",
    "    \"\"\"Genera un JSON di esempio se non hai il modello addestrato\"\"\"\n",
    "    print(\"🎭 Generazione JSON mock per test...\")\n",
    "    \n",
    "    gallery_paths = get_image_paths(Config.gallery_dir)\n",
    "    query_paths = get_image_paths(Config.query_dir)\n",
    "    \n",
    "    if not gallery_paths or not query_paths:\n",
    "        print(\"❌ Directory gallery o query non trovate\")\n",
    "        return\n",
    "    \n",
    "    results = []\n",
    "    for query_path in query_paths:\n",
    "        # Seleziona immagini casuali dalla gallery (senza ripetizioni)\n",
    "        similar_images = random.sample(gallery_paths, min(Config.top_k, len(gallery_paths)))\n",
    "        \n",
    "        # Formato richiesto: filename + gallery_images  \n",
    "        results.append({\n",
    "            \"filename\": query_path,\n",
    "            \"gallery_images\": similar_images\n",
    "        })\n",
    "    \n",
    "    # Salva JSON mock\n",
    "    mock_filename = 'submission_mock.json'\n",
    "    with open(mock_filename, 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    print(f\"✅ JSON mock generato: {mock_filename}\")\n",
    "    return results\n",
    "\n",
    "# Decommenta per generare dati mock se non hai il modello\n",
    "# generate_mock_json()\n",
    "\n",
    "# ================================\n",
    "# ESEMPI DI UTILIZZO\n",
    "# ================================\n",
    "\"\"\"\n",
    "ESEMPI DI COME MODIFICARE I PARAMETRI:\n",
    "\n",
    "1. Modifica parametri singoli:\n",
    "   Config.batch_size = 32\n",
    "   Config.num_epochs = 10\n",
    "   Config.learning_rate = 5e-5\n",
    "\n",
    "2. Modifica parametri multipli:\n",
    "   Config.update_training_params(\n",
    "       batch_size=32,\n",
    "       num_epochs=10, \n",
    "       learning_rate=5e-5,\n",
    "       embedding_dim=512\n",
    "   )\n",
    "\n",
    "3. Usa preset predefiniti:\n",
    "   training_preset_light()     # Training veloce\n",
    "   training_preset_heavy()     # Training intensivo\n",
    "   training_preset_balanced()  # Training bilanciato\n",
    "\n",
    "4. Modifica rapida:\n",
    "   quick_config_update(num_epochs=15, batch_size=8)\n",
    "\n",
    "5. Visualizza configurazione corrente:\n",
    "   Config.print_config()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e84e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# per visualizzare\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# Load submission file\n",
    "with open('submission7.json', 'r') as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "# How many queries to show?\n",
    "num_queries_to_show = 5\n",
    "top_k = len(results[0]['gallery_images'])\n",
    "\n",
    "# Plot\n",
    "for i, item in enumerate(results[:num_queries_to_show]):\n",
    "    query_img = Image.open(item['filename']).convert('RGB')\n",
    "\n",
    "    fig, axs = plt.subplots(1, top_k + 1, figsize=(3 * (top_k + 1), 4))\n",
    "    axs[0].imshow(query_img)\n",
    "    axs[0].set_title(\"Query\")\n",
    "    axs[0].axis('off')\n",
    "\n",
    "    for j, gallery_path in enumerate(item['gallery_images']):\n",
    "        gallery_img = Image.open(gallery_path).convert('RGB')\n",
    "        axs[j + 1].imshow(gallery_img)\n",
    "        axs[j + 1].set_title(f\"Top {j+1}\")\n",
    "        axs[j + 1].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67763a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import faiss\n",
    "import numpy as np\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "from torchvision import transforms, models\n",
    "from tqdm import tqdm\n",
    "\n",
    "# =========================\n",
    "# CONFIGURAZIONE\n",
    "# =========================\n",
    "gallery_folder = 'test/gallery'\n",
    "query_folder = 'test/query'\n",
    "top_k = 10\n",
    "\n",
    "# Percorsi per caching\n",
    "features_file = 'gallery_features.npy'\n",
    "paths_file = 'gallery_paths.txt'\n",
    "\n",
    "# Usa GPU se disponibile\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# =========================\n",
    "# MODELLO VELOCE: ResNet18\n",
    "# =========================\n",
    "model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "model = torch.nn.Sequential(*list(model.children())[:-1])  # Rimuove FC\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# =========================\n",
    "# TRASFORMAZIONE\n",
    "# =========================\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# =========================\n",
    "# ESTRAZIONE FEATURE\n",
    "# =========================\n",
    "def extract_features(image_path):\n",
    "    try:\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "    except UnidentifiedImageError:\n",
    "        print(f\"[ERRORE] Immagine non valida: {image_path}\")\n",
    "        return None\n",
    "    image = transform(image).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        feature = model(image).squeeze().cpu().numpy()\n",
    "    return feature\n",
    "\n",
    "# =========================\n",
    "# CARICA o CALCOLA GALLERIA\n",
    "# =========================\n",
    "if os.path.exists(features_file) and os.path.exists(paths_file):\n",
    "    print(\"📂 Caricamento feature galleria salvate...\")\n",
    "    gallery_features = np.load(features_file)\n",
    "    with open(paths_file, 'r') as f:\n",
    "        valid_gallery_paths = [line.strip() for line in f.readlines()]\n",
    "else:\n",
    "    print(\"🧠 Estrazione feature dalla galleria...\")\n",
    "    gallery_paths = [os.path.join(gallery_folder, fname)\n",
    "                     for fname in os.listdir(gallery_folder)\n",
    "                     if fname.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "\n",
    "    gallery_features = []\n",
    "    valid_gallery_paths = []\n",
    "\n",
    "    for p in tqdm(gallery_paths, desc=\"Estrai galleria\"):\n",
    "        feat = extract_features(p)\n",
    "        if feat is not None:\n",
    "            gallery_features.append(feat)\n",
    "            valid_gallery_paths.append(p)\n",
    "\n",
    "    gallery_features = np.array(gallery_features).astype('float32')\n",
    "    np.save(features_file, gallery_features)\n",
    "    with open(paths_file, 'w') as f:\n",
    "        for path in valid_gallery_paths:\n",
    "            f.write(path + '\\n')\n",
    "\n",
    "# =========================\n",
    "# COSTRUISCI INDICE FAISS\n",
    "# =========================\n",
    "index = faiss.IndexFlatL2(gallery_features.shape[1])\n",
    "index.add(gallery_features)\n",
    "\n",
    "# =========================\n",
    "# ELABORA QUERY\n",
    "# =========================\n",
    "results = []\n",
    "query_paths = [os.path.join(query_folder, fname)\n",
    "               for fname in os.listdir(query_folder)\n",
    "               if fname.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "\n",
    "top_k_effettivo = min(top_k, len(valid_gallery_paths))\n",
    "\n",
    "print(\"🔍 Ricerca immagini simili...\")\n",
    "for q_path in tqdm(query_paths, desc=\"Query\"):\n",
    "    q_feat = extract_features(q_path)\n",
    "    if q_feat is None:\n",
    "        print(f\"[ATTENZIONE] Query ignorata: {q_path}\")\n",
    "        continue\n",
    "\n",
    "    q_feat = q_feat.astype('float32').reshape(1, -1)\n",
    "    distances, indices = index.search(q_feat, top_k_effettivo)\n",
    "    similar_images = [valid_gallery_paths[i] for i in indices[0]]\n",
    "    results.append({\n",
    "        \"filename\": q_path,\n",
    "        \"gallery_images\": similar_images\n",
    "    })\n",
    "\n",
    "# =========================\n",
    "# SALVA RISULTATI\n",
    "# =========================\n",
    "with open('submission_results.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(\"✅ Ricerca completata. File salvato: submission_results.json\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
